data:
  target: datasets.data_module.BaseDataModule
  params:
    # Path to training set configuration file.
    train_config: configs/datasets/realsr_train.yaml
    # Path to validation set configuration file.
    val_config: configs/datasets/realsr_test_allscale.yaml

model:
  # You can set learning rate in the following configuration file.
  config: configs/models/ae.yaml
  # Path to the checkpoints or weights you want to resume. At the begining, 
  resume: ~

lightning:
  seed: ~ # 42
  mode: fit # fit # validate
  
  trainer:
    accelerator: auto # gpu # ddp
    precision: 32 # bf16-mixed # 32 # 16-mixed
    # Indices of GPUs used for training.
    devices: [1, 3, 5, 7] 
    # For Multi GPU
    strategy: auto
    # Max number of training steps (batches).
    max_steps: 200001
    # Validation frequency in terms of training steps.
    val_check_interval: 20000 #20000
    check_val_every_n_epoch: ~
    log_every_n_steps: 20000 #100000
    # Accumulate gradients from multiple batches so as to increase batch size.
    accumulate_grad_batches: 1

  callbacks:
    - target: trainers.model_checkpoint.AutoencoderCheckpoint
      params:
        save_dir: "./checkpoints/autoencoder/"
        save_interval: 200000
        save_last: False
        verbose: True

    - target: pytorch_lightning.callbacks.LearningRateMonitor
      params:
        logging_interval: 'step'
        
    - target: pytorch_lightning.callbacks.RichProgressBar
      params:
        leave: True

  loggers:
    - target: pytorch_lightning.loggers.TensorBoardLogger
      params:
        save_dir : ./logs/
        version: 1

